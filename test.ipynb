{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\anaconda3\\envs\\bigdl-prompt-engineer\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-22 10:59:09,582 - WARNING - BigdlNativeLLM has been deprecated, please switch to the new LLM API for sepcific models.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "from bigdl.llm.langchain.llms import TransformersLLM\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import ConversationChain, LLMChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "\n",
    "MODEL_NAME = \"lmsys-vicuna-7b-v1.5\"\n",
    "\n",
    "def load_transformers_llm(model_name = MODEL_NAME):\n",
    "    # Define the base folder path\n",
    "    base_folder_path = Path(\"F:/Study/Code/llm-models\")\n",
    "\n",
    "    # Append MODEL_NAME to the folder path\n",
    "    model_path = base_folder_path + \"/\" + model_name\n",
    "\n",
    "    return TransformersLLM.from_model_id_low_bit(\n",
    "        model_id=model_path,\n",
    "        model_kwargs={\"temperature\": 0.9, \"trust_remote_code\": True},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mload_transformers_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m, in \u001b[0;36mload_transformers_llm\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_transformers_llm\u001b[39m(model_name \u001b[38;5;241m=\u001b[39m MODEL_NAME):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Define the base folder path\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     base_folder_path \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF:/Study/Code/llm-models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Append MODEL_NAME to the folder path\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m base_folder_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = load_transformers_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a chatbot having a conversation with a human.\"), # The persistent system prompt\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # Where the memory will be stored.\n",
    "    HumanMessagePromptTemplate.from_template(\"我的问题是：```{human_input}```。如果我的问题不满足语法，请完善这句话，并返回完善过的问题\"), # Where the human input will injected\n",
    "])\n",
    "    \n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    "    llm_kwargs={\"max_new_tokens\": 500}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm_chain.predict(human_input=\"复旦大学 地点？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
